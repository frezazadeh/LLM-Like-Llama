# LLM-Like-Llama

ğŸ“‚ Project Structure

llm_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ config.py
â”œâ”€â”€ data_utils.py
â”œâ”€â”€ tokenizer.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train.py
â”œâ”€â”€ inference.py
â””â”€â”€ main.py

1ï¸âƒ£ README.md

This README.md provides setup instructions and commands to run the project.

# ğŸ§  LLM Training and Inference

This project contains a small LLM (Large Language Model) that can be trained and used for text generation.

---

## ğŸš€ **Setup**
Before running anything, **install dependencies**:

```bash
pip install torch sentencepiece tqdm

ğŸ”— Step 1: Tokenizer Preparation

Before training the model, you must prepare the tokenizer:

python tokenizer.py

This step ensures that the tokenizer is trained and ready to be used.
ğŸ“ˆ Step 2: Training the Model

To train the model, run:

python main.py --DEVICE cuda --mode train

If you want to train on CPU, run:

python main.py --DEVICE cpu --mode train

ğŸ¤– Step 3: Running Inference

Once trained, you can generate text:

python main.py --DEVICE cuda --mode inference

If you trained the model on CPU, you can also infer on CPU:

python main.py --DEVICE cpu --mode inference

